- [[毕业设计]]
	- HAR任务综述：[用于骨骼动作识别的卷积网络模型——综述](https://blog.csdn.net/xiaoyuting999/article/details/129903796?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-129903796-blog-121403263.235^v38^pc_relevant_anti_t3_base&spm=1001.2101.3001.4242.2&utm_relevant_index=4)
	- 思考：空域和谱域的关系？[图神经网络谱域与空域的对比](https://zhuanlan.zhihu.com/p/148835806)
	- 感悟：
		- 阅读开创文献
		- 同时很多其他领域的文献也有帮助
		-
	- 背景知识
		- HAR相关
			- [A Gentle Introduction to Graph Neural Networks (distill.pub)](https://distill.pub/2021/gnn-intro/) -- 图神经网络超棒入门
				- ST-GCN的关键理解在笔记本上 -- ST-GCN类比到了CNN模型之中
					- ST-GCN 代码链接：[ST-GCN论文与代码解析](https://zhuanlan.zhihu.com/p/418989078)
					- ST-GCN**最难理解的一部分**[ST-GCN中关于GCN部分的讲解](https://www.zhihu.com/question/324532901/answer/754377409)
						- 这也是一个关于这部分GCN的一个讲解：[另一个讲解](https://blog.csdn.net/qq_44024204/article/details/115030327)
					- ```
					  class ConvTemporalGraphical(nn.Module):
					      def __init__(self,
					                   in_channels,
					                   out_channels,
					                   kernel_size,
					                   t_kernel_size=1,
					                   t_stride=1,
					                   t_padding=0,
					                   t_dilation=1,
					                   bias=True):
					          super().__init__()
					  
					          self.kernel_size = kernel_size
					          self.conv = nn.Conv2d(
					              in_channels,
					              out_channels * kernel_size,
					              kernel_size=(t_kernel_size, 1),
					              padding=(t_padding, 0),
					              stride=(t_stride, 1),
					              dilation=(t_dilation, 1),
					              bias=bias)
					   注意这里面的kernel_size不是定义的self.conv卷积层的kernel_size，
					   而是论文中多子集邻接矩阵的子集数，和你的子集分组策略有关，分多少个组就有多少，
					   Uni-labeling是1，Distance partitioning是2，
					   Spatial configuration partitioning是3。代码中kernel_size = A.size(0)，
					   A是(K, V, V)的邻接矩阵，那就是这个K了。
					   在此K也被称为Size of the graph convolving kernel，
					   这一段中的kernel_size，子集分组数，A.size(0)，
					   A的形状(K, V, V)中的K这些都是一个东西。
					      def forward(self, x, A):
					          assert A.size(0) == self.kernel_size
					  
					          x = self.conv(x)
					          
					          
					  而这里的self.conv的卷积核真正的大小是(t_kernel_size, 1)，
					  t_kernel_size预设值是1，那么就是一个1x1的卷积层，
					  在第一层时就相当于把输入的(512, 3, 150, 18)转变成了(512, output_channels * kernel_size, 150, 18)，
					  第一层中output_channels为64，kernel_size是[1, 2, 3]中的一个视分区策略而定。
					  这一层1x1卷积只是把特征升维，且按你子集分了多少组加倍了，
					  在x = self.conv(x)之后是
					  看链接！这里写不下了。
					          n, kc, t, v = x.size()
					          x = x.view(n, self.kernel_size, kc//self.kernel_size, t, v)
					          x = torch.einsum('nkctv,kvw->nctw', (x, A))
					  
					          return x.contiguous(), A
					         
					  ```
				- 接下来是tcn的卷积，tcn的卷积输入是N*C*T*V,对应con2D中的(batchsize,channel,height,width)，所以tcn中卷积核的大小是(9,1)，表示的也是(height,width)，所以，是在时间维度进行卷积。
			- 2s-AGCN相关链接
				- $C_K$:类似于视觉中的Non - Local Network：[Non-local，长距离非局部依赖块](https://www.bilibili.com/video/BV1Be4y1g7t9/?spm_id_from=333.337.search-card.all.click&vd_source=19d43a740c787dccf39bbc5751d77b0c)
				  其实就是一个长距离注意力机制，这个视频很好的解释了。矩阵的reshape操作是为了计算全局元素之间的关系。
				- B-stream:每个骨头也可以用节点来表示，其中B图比J图少一个节点，用0节点表示，从B、J拥有相同的结构和网络
			- AS-GCN
				- [AS-GCN 论文解读-CSDN博客](https://blog.csdn.net/xiaoyuting999/article/details/130039277)
				- Gumbel softmax？[通俗易懂地理解Gumbel Softmax ](https://zhuanlan.zhihu.com/p/633431594)**没有太看懂**
				- **A-Link？**
					- 所谓的C种链接类型，是一个固定的值，还是一个最后计算出来的种类
					- encoder和decoder的多层感知器并不分享一样的参数
			- MS-AAGCN
				- GCN部分和2s-AGCN不同的是
					- ![image.png](../assets/image_1696917424605_0.png)
					  其中$B_K$是一开始是$A_K$初始化，但是是可学习矩阵，整个图的数据不再约束
				- 其中的注意力机制
					- 空域、时间域、通道域的注意力
						- 类似于CBAM
			- 1*1卷积的作用：升高维度或者降低维度
	- 论文阅读
		- ![Sparse_Adaptive_Graph_Convolutional_Network_for_Leg_Agility_Assessment](../assets/Sparse_Adaptive_Graph_Convolutional_Network_for_Leg_Agility_Assessment_in_Parkinsons_Disease_1696926180602_0.pdf)
			- TCM
				- 类似于non-local network的操作，得到一个时间上的注意力机制
		- ![Multi-Scale_Sparse_Graph_Convolutional_Network](../assets/Multi-Scale_Sparse_Graph_Convolutional_Network_For_the_Assessment_of_Parkinsonian_Gait_1696926215873_0.pdf)
			- 2s-ST-AGCN
				- J-stream and B-stream
			- STAM
				- 高级与低级特征联合处理为注意图
			- L1和L2正则化
			- 学习
				- 1*1矩阵的作用
				- L1正则化的稀疏作用
		- ![A_Contrastive_Graph_Convolutional_Network_for_Toe-Tapping_Assessment](../assets/A_Contrastive_Graph_Convolutional_Network_for_Toe-Tapping_Assessment_in_Parkinsons_Disease_1696926243073_0.pdf)
			- **这篇文章很重要，对于细微的异质性和相同的冗余性分析很有启发。通过脚趾拍地来判别PD患者的评级是一件困难的事情，也是一个有难度的细粒度任务。基于先验的！**
			- 对比学习
				- 旨在通过最**大化相关样本之间的相似性并最小化不相关样本之间的相似性来学习数据表示**。通常使用一种高自由度、自定义的规则生成正负样本。在模型预训练中有着广泛的应用。
				- 自监督学习(Self-supervised learning)可以避免对数据集进行大量的标签标注。**把自己定义的伪标签当作训练的信号，**然后把学习到的表示(representation)用作下游任务里。最近，对比学习被当作自监督学习中一个非常重要的一部分，被广泛运用在计算机视觉、自然语言处理等领域。它的目标是：**将一个样本的不同的、增强过的新样本们在嵌入空间中尽可能地近，然后让不同的样本之间尽可能地远。**
	- 代码？ #毕业问题
-
-
-